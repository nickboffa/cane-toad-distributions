---
title: "Data Cleaning"
author: "Nicholas Boffa u7686660"
date: "2024-04-28"
output: html_document
---

# Load Libraries

```{r load-libraries, message=F}
library(tidyverse)
library(raster)
library(knitr)
library(dbscan)
library(terra)
library(geodata)
library(sf)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = TRUE)
```
# Get WorldClim data

Data can be downloaded by running 

output_dir <- "raw_data/wc2.1_10m"
bioclim_data <- geodata::worldclim_global(var = "bio", res = 10, path = output_dir)


```{r}
bio_files <- list.files("raw_data/climate/wc2.1_10m", full.names = TRUE)

bioclim_names <- c(
  "Annual Mean Temperature",            # bio1
  "Mean Diurnal Range",                 # bio2
  "Isothermality",                      # bio3
  "Temperature Seasonality",            # bio4
  "Max Temperature of Warmest Month",   # bio5
  "Min Temperature of Coldest Month",   # bio6
  "Temperature Annual Range",           # bio7
  "Mean Temperature of Wettest Quarter",# bio8
  "Mean Temperature of Driest Quarter", # bio9
  "Mean Temperature of Warmest Quarter",# bio10
  "Mean Temperature of Coldest Quarter",# bio11
  "Annual Precipitation",               # bio12
  "Precipitation of Wettest Month",     # bio13
  "Precipitation of Driest Month",      # bio14
  "Precipitation Seasonality",          # bio15
  "Precipitation of Wettest Quarter",   # bio16
  "Precipitation of Driest Quarter",    # bio17
  "Precipitation of Warmest Quarter",   # bio18
  "Precipitation of Coldest Quarter"    # bio19
)

# Read the bioclimatic layers into a RasterStack
bioclim_stack <- stack(bio_files)

# Assign the new names to the layers in the bioclim_stack
names(bioclim_stack) <- bioclim_names

xmin <- 113.0  # Westernmost point of mainland Australia
xmax <- 154.0  # Easternmost point of mainland Australia
ymin <- -44.0  # Southernmost point of mainland Australia
ymax <- -10.0  # Northernmost point of mainland Australia

base_extent <- c(xmin, xmax, ymin, ymax)

bioclim_stack <- crop(bioclim_stack, base_extent)
```



```{r}
australia_gadm <- gadm(country = "AUS", level = 0, path = tempdir())

# Convert to sf object and cast to polygons
australia_sf <- australia_gadm %>%
  st_as_sf() %>%
  st_cast("POLYGON")

# Calculate area and select the largest polygon (mainland Australia)
mainland_australia <- australia_sf %>%
  mutate(area = st_area(.)) %>%
  arrange(desc(area)) %>%
  slice(1)

# Convert the map to a Spatial object for masking
australia_spatial <- as(mainland_australia, "Spatial")

st_write(mainland_australia, "processed_data/mainland_australia.shp")
```


```{r}
# Mask the bioclim_stack with the mainland Australia shapefile
bioclim_stack_masked <- mask(bioclim_stack, australia_spatial)
plot(bioclim_stack_masked)
writeRaster(bioclim_stack_masked, "processed_data/bioclim_stack", filetype="GTiff", overwrite=TRUE) 
```




# Get Image Raster Data

```{r}
src_image <- "raw_data/breeding.png"
dst_image <- "processed_data/georef_breeding_image.tif"

image_raster <- rast(src_image) # have to use terra for image processing
ext(image_raster) <- base_extent
crs(image_raster) <- "WGS84"

writeRaster(image_raster, dst_image, filetype="GTiff", overwrite=TRUE)
```


```{r}
image_raster <- rast(dst_image)
plot(image_raster)
```

Remove text from image using whatever works

```{r}
# Extract the RGB channels
red_channel <- raster(image_raster[[1]])
green_channel <- raster(image_raster[[2]])
blue_channel <- raster(image_raster[[3]])

black_mask <- (red_channel < 100 | green_channel < 100 | blue_channel > 100)

# Apply the mask to set black pixels to NA
red_channel[black_mask] <- NA
green_channel[black_mask] <- NA
blue_channel[black_mask] <- NA
blue_channel[blue_channel > 250] <- NA
blue_channel[is.na(blue_channel)] <- NA

plot(blue_channel) # green channel also works!?
```

```{r}
agged <- aggregate(blue_channel, fact=6, fun=median)
breeding <- !is.na(agged)
lat_mask <- breeding
lat_mask <- init(lat_mask, "y") > -20

breeding_corrected <- breeding
breeding_corrected[lat_mask == 1] <- 1
breeding_corrected <- mask(breeding_corrected, australia_spatial)
plot(breeding_corrected)
plot(australia_spatial, add = TRUE, border = "red")
```

Finally, we can see that there are some gaps still remaining

```{r}
plot(breeding_corrected == 0)
```

And so we fill them in


```{r}
fill_cells <- function(x) {
  if (is.na(x[5])) {
    return(NA)
  } else if (x[5] == 0 & all(x[-5] == 1 | is.na(x[-5]))) {
      return(1)
  } else {
      return(x[5])
  }
}

# Apply the function to the raster using focal
filled_raster <- focal(breeding_corrected, w=matrix(1, 3, 3), fun=fill_cells, na.policy='omit', pad=TRUE, padValue=0)

# Plot the original and filled rasters for comparison
par(mfrow=c(1,2))
plot(breeding_corrected, main="Original Raster")
plot(filled_raster, main="Filled Raster")
```

```{r}
plot(filled_raster == 0)
```

```{r}
final <- resample(filled_raster, bioclim_stack[[1]]) > 0.9
```

```{r}
plot(final)
```



Not perfect, but pretty good

```{r}
raster::writeRaster(final, "processed_data/breeding.grd", overwrite=TRUE)
```
# Create australia_raster for ABM


```{r}
degree_per_km <- 1 / 111
km_per_cell <- 40
degree_per_cell <- degree_per_km * km_per_cell
agg_factor <- degree_per_cell / res(bioclim_stack[[1]])

# Aggregate the raster to the new resolution
mainland <- mask(bioclim_stack[["Isothermality"]], australia_spatial)
mainland_agg <- aggregate(mainland, fact = agg_factor)

#australia_raster <- !is.na(mainland_agg)
plot(australia_raster)
```

```{r}
writeRaster(australia_raster, "processed_data/australia_raster", filetype="GTiff", overwrite=TRUE)
```

# Prepare Observational Data

## Load raw data

Downloaded by ALA, with default settings on (e.g. spatially suspect records were excluded)
```{r}
toads <- read_csv("raw_data/rhinella-marina-occurence.csv")
```
Could geo-reference the missing coordinate observations using the 'locality' column






## Data Tidying 

This data is already tidy. However, we will take this opportunity to remove most of the columns in toads, and also give each row a unique identifier

```{r}
head(tidy_df)
```

```{r}
tidy_df <- toads |> 
  dplyr::select(eventDate, year, stateProvince, locality, decimalLongitude, decimalLatitude,) |> 
  arrange(eventDate)

tidy_df$id <- 1:nrow(tidy_df)

tidy_df <- tidy_df |> 
  rename(longitude = decimalLongitude,
         latitude = decimalLatitude)

head(tidy_df)

write_csv(tidy_df, "processed_data/tidied_data.csv")
```

## Make Excluded Data File


### Data Cleaning
#### Remove rows with NA locations


```{r}
clean <- tidy_df |> 
  filter(!is.na(latitude) & !is.na(longitude))
clean
```


```{r}
excluded_part <- anti_join(tidy_df, clean, by = c("id", "longitude", "latitude"))
excluded_part$reason <- rep("Undefined longitude or latitude", nrow(excluded_part))
excluded_df <- excluded_part
excluded_df
```


We could theoretically georeference these locations, but we have so many records that this isn't worth the effort. There are only 73 rows that could be georeferenced anyway, and according to https://docs.gbif.org/georeferencing-best-practices/1.0/en/ however, it is better to not georeference at all than georeference poorly.


#### Remove observations that aren't in mainland Australia

```{r}
before_removal <- clean
```

```{r}
r <- mainland
xmin_new <- xmin(r) - 10
xmax_new <- xmax(r) + 10
ymin_new <- ymin(r) - 10
ymax_new <- ymax(r) + 10

new_extent <- extent(xmin_new, xmax_new, ymin_new, ymax_new)

# Extend the raster
extended_mainland <- raster::extend(r, new_extent, value = NA)

fill_cells2 <- function(x) {
  if (any(!is.na(x)) | any(x == 1, na.rm=T)) {
    return(TRUE)
  } else {
      return(FALSE) # centre
  }
}

# Apply the function to the raster using focal
padded_mainland <- focal(extended_mainland, w=matrix(1, 13, 13), fun=fill_cells2, na.policy='omit', pad=TRUE)
plot(padded_mainland)
```


```{r}
sp_clean <- clean
coordinates(sp_clean) <- c("longitude", "latitude")
is_land <- raster::extract(padded_mainland, sp_clean)

# Filter rows in the spatial data frame based on the raster values
clean <- clean[which(is_land > 0),]
```


```{r}
excluded_part <- anti_join(before_removal, clean, by = c("id", "longitude", "latitude"))
excluded_part$reason <- rep("Not in mainland Australia", nrow(excluded_part))
excluded_df <- rbind(excluded_df, excluded_part)
```



#### Remove rows that aren't in the main cluster

```{r}
before_removal <- clean
```

```{r}
clusters <- dbscan(clean[, c("longitude", "latitude")], eps = 2.5, minPts = 3) # arguments heavily affect result

# Add cluster IDs to your data frame
clean$cluster <- clusters$cluster

# Create a ggplot of the clusters
ggplot(clean[, c("longitude", "latitude", "cluster")], aes(x = longitude, y = latitude, color = as.factor(cluster))) +
  geom_point(aes(size = 3), alpha = 0.6) +
  labs(title = "DBSCAN Clustering of Geographical Data", x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(legend.position='none')
```

The yellow matches current distribution, unlike the cleaned data https://www.researchgate.net/publication/320257212_Effects_of_invasion_history_on_physiological_responses_to_immune_system_activation_in_invasive_Australian_cane_toads

```{r}
is_large <- table(clean$cluster) > 100
large_clusters <- is_large[is_large]

clean <- clean |> 
  filter(cluster %in% large_clusters)
```

```{r}
excluded_part <- anti_join(before_removal, clean, by = c("id", "longitude", "latitude"))
excluded_part$reason <- rep("In mainland Australia, but not in known cane toad habitat", nrow(excluded_part))
excluded_df <- rbind(excluded_df, excluded_part)
```

```{r}
excluded_df
```




#### Not removing spatio-temporally inaccurate data

As we will find in the main project, some observations appear to be in the correct kind of place, but at the wrong time. For example, there are observations from 01-01-1900 in Brisbane that must be wrong, because Cane Toads first arrived in Australia in 1935. 

```{r}
head(df, 3)
```

Whilst we could remove these, for the purposes of training our model (which doesn't use the date) they are fine, since they are still in the correct location. Additionally, no spatio-temporal errors appear to be in WA, which is where we most don't want them (since then they might be included in the data for before 2000, defeating the purpose of limiting the data in such a way).

To see what I'm talking about, see the final 'model_plot' from the final_project, that compares the model's output to reality.



#### Summary

```{r}
unique_points <- anti_join(tidy_df, clean, by = c("id", "longitude", "latitude"))

ggplot() +
  geom_point(data = toads, aes(x = decimalLongitude, y = decimalLatitude), color = "black") +
  geom_point(data = unique_points, aes(x = longitude, y = latitude), color = "red")
```

And zooming in just on Australia:

```{r}
ggplot() +
  geom_point(data = toads, aes(x = decimalLongitude, y = decimalLatitude), color = "black") +
  geom_point(data = unique_points, aes(x = longitude, y = latitude), color = "red") +
  xlim(100, 160) +
  ylim(-40, -5)
```

```{r}
write_csv(excluded_df, "processed_data/excluded_data.csv")
```


## Data Exclusion


```{r}
cleaned_df <- anti_join(tidy_df, excluded_df, by = "id")

write_csv(cleaned_df, "processed_data/cleaned_data.csv")

```







